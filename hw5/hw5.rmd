---
title: "Academy 2.0, Homework 5"
author: "Hüseyin Çavuş"
date: "2024-04-22"
output:
  word_document: default
  html_document: default
  pdf_document: default
 R 4.3.3
 ggplot2 3.5.0
 rpart 4.1.23
 rpart.plot 3.1.2
 zoo 1.8-12
 forecast 8.22.0
---

HW5.1
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Creating data
set.seed(123) 
fertilizer_A_growth_rate <- rnorm(30, mean=5, sd=1.5) 
fertilizer_B_growth_rate <- rnorm(30, mean=6, sd=1.5) 

# Creating histogram for visualization 
hist(fertilizer_A_growth_rate, col="purple", main="Fertilizer A's Growth Rates", xlab="Growth Rate")
hist(fertilizer_B_growth_rate, col="green", main="Fertilizer B's Growth Rates", xlab="Growth Rate") # The `add=TRUE` argument is used to add the histogram to an EXISTING plot

# Two sets of data are plotted on the same histogram
hist(fertilizer_A_growth_rate, col="purple", main="Histogram for Fertilizer A and B Growth Rates", xlab="Growth Rates", xlim=c(2,8))

hist(fertilizer_B_growth_rate, col=adjustcolor("green", alpha.f=0.5), add=TRUE, xlim=c(2,10))
legend("topright", legend=c("Fertilzer A's Growth Rates", "Fertilizer B's Growth Rates"), fill=c("purple", adjustcolor("green", alpha.f=0.5)))

# Independent two-sample t-test to determine whether the mean difference is statistically significant
test_result <- t.test(fertilizer_A_growth_rate, fertilizer_B_growth_rate)
print(test_result)
```

 Welch Two Sample t-test

data:  fertilizer_A_growth_rate and fertilizer_B_growth_rate
t = -3.7926, df = 56.559, p-value = 0.0003646; df is (n1 + n2 - 2) for two samp t-test, but df = 56.6, so some of the data points appear to have the same values.
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.0448139 -0.6315124
sample estimates:
mean of x mean of y 
 4.929344  6.267508 

 > Negative t value indicates that the mean of the Fertilizer A is less than the mean of the Fertilizer B. The p-value is less than 0.05.
 > The null hypothesis is rejected. The alternative hypothesis points to a difference.
 > 95 percent confidence interval supports that there is a significant difference between FertA and B.
 > FertB is better than FertA according to sample estimates.
```

## HW5.2
```{r}

## 1 Histogram
set.seed(9)
data = rnorm(100, 30, 30) # mean = 30, sd = 30
View(data)
max(data)
min(data)
hist(data, col = "darkblue", border = "black", main = "Histogram of data", xlab = "Data", ylab = "Frequency", breaks = 15, xlim=c(-50, 150))

```

```{r}
## 2 Analysing Phase 1 Clinical Trial Data of a New Drug Candidate with Chi-Square Test
age_group <- c("18-40", "18-40", "18-40", "41-69", "41-69", "41-69", "70+", "70+", "70+")
drug_cand <- c("Drug", "Drug", "Placebo", "Drug", "Drug", "Placebo", "Drug", "Drug", "Placebo")
is_safe <- c("Yes", "Yes", "No", "Yes", "No", "No", "No", "No", "No")
# Evaluating the safety of a new drug candidate of Young/Middle/Old patients 

data_frame <- data.frame(age_group = age_group, drug_cand = drug_cand, is_safe = is_safe)
data_frame
sapply(data_frame, class)

# Chi-square test of independence
result <- chisq.test(data_frame$drug_cand, data_frame$is_safe)
result

```
The larger the X-squared statistic, the greater the association between the variables. Unfortunately, it does not give a conclusion that the drug candidate is safe. The high p value also indicates that there is no statistically significant relationship. 

```{r}
## 3 Correlation
set.seed(1)
x = rnorm(100, 30, 30)
y = 2*x + rnorm(100, 0, 10)

# plot only for x
plot(x, col = "blue", pch = 19, main = "Scatterplot of x", xlab = "x", ylab = "Frequency")

# plot only for y
plot(y, col = "pink", pch = 19, main = "Scatterplot of y", xlab = "y", ylab = "Frequency")

cor(x, y)
plot(x, y, col = "red", pch = 19, main = "Scatterplot of x and y", xlab = "x", ylab = "y") # pch controls the type of symbol
abline(lm(y ~ x), col = "yellow", lwd = 4) # add a regression line
```
correlation coefficient indicates a strong positive relationship. The regression line is also plotted on the scatterplot with positive slope. 


```{r}
## 4 ANOVA for Testing Two New Drug's Effects on Blood Glucose Levels 
drug_a <- c(90, 115, 102, 118, 147) # Blood sugar levels of patients who took Drug A
drug_b <- c(122, 130, 165, 188, 129) # Blood sugar levels of patients who took Drug B
placebo <- c(79, 115, 92, 128, 145) # Blood sugar levels of patients who took Placebo

data_frame <- data.frame(drug_a, drug_b, placebo)
str(data_frame)

result <- aov(data_frame$placebo ~ data_frame$drug_a + data_frame$drug_b)
summary(result)

```
A high F-value with low p-value indicates that the variance between groups is different and not random. Drug_a significantly affects blood glucose levels.

High p-value and low F-value indicates that drug_b does not cause a significant difference. Drug_b does not significantly affect blood glucose levels.

```{r}
## 5 Linear Regression 
set.seed(19)
x = rnorm(200, 75, 50)
y = x + rnorm(50, 1, 10)

model <- lm(y ~ x)
summary(model)

```
x affects y positively and strongly. An increase of x by 1 unit is estimated to cause an increase of 1.02530 units in y.  The model explains the data quite well (R-sq 0.9638). The F-statistic value is high and the p-value is very small. The model is significant.


```{r}
## 6 Logistic Regression for Predicting Cancer Diagnosis
cancer_data <- data.frame(
  cancer = c(1, 0, 1, 0, 1),  # 0 or 1, No or Yes
  age = c(55, 62, 47, 68, 35),  
  sex = c("M", "F", "M", "F", "M"), 
  smoking = c("Yes", "No", "Yes", "No", "No"), 
  family_history = c("Yes", "No", "No", "Yes", "No")  
)

cancer_data$sex <- factor(cancer_data$sex, levels = c("M", "F"))
cancer_data$smoking <- factor(cancer_data$smoking, levels = c("Yes", "No"))
cancer_data$family_history <- factor(cancer_data$family_history, levels = c("Yes", "No"))
cancer_data

model <- glm(cancer ~ age + sex + smoking + family_history, data = cancer_data, family = "binomial") # family = binominal represents a distribution with two possible outcomes
summary(model)
```
When we examine the coefficients of this model, it seems that higher age, not smoking(?), not having a family history of cancer and being male increase the probability of having cancer. However, the standard errors and p-values are quite high. More work is needed in this area. The low residual deviance value shows that the model fits well when using the independent variables. However, the predictive ability of the model without using the independent variable (cancer diagnosis) is quite low (high null deviance value). The model also has a low AIC value (assesses the fit and complexity of the model) and a medium Fisher Scoring Iteration number (the number of iterations to optimise the model's predictions). Overall, we can conclude that the model is not very reliable.

```{r}
## 7 Effects of Sample Size
# Small Sample
set.seed(8)  
sample_means <- sapply(1:1000, function(x) mean(rnorm(x)))

hist(sample_means, breaks = 30, main = "Distribution of The Means of 1000 Samples", xlab = "Mean", col = "skyblue", border = "black", xlim = c(-2, 2))

# Big Sample
sample_means <- sapply(1:10000, function(x) mean(rnorm(x)))
hist(sample_means, breaks = 50, main = "Distribution of The Means of 100000 Samples", xlab = "Mean", col = "skyblue", border = "black", xlim = c(-2, 2))
```
When we increase the sample size, the distribution of sample means becomes closer to the normal distribution and more symmetric. 

```{r}
## 8 %95 Confidence Interval
bmi_cand <- read.csv("/home/macbuntu/Downloads/Academy/insurance.csv") 
# https://www.kaggle.com/datasets/willianoliveiragibin/healthcare-insurance?resource=download

bmi <- bmi_cand$bmi
mean(bmi)

# Calculate the 95% confidence interval
bmi_ci <- t.test(bmi)$conf.int # default >> conf.level = 0.95
bmi_ci
```
The mean of the BMI variable in the set of 7 variables of 1338 individuals was calculated. The possible true value of the mean parameter of the BMI variable at the 95% confidence level was calculated. Accordingly, the mean BMI values of these individuals are between 30.3 and 31.0. individuals in this set are generally overweight.


```{r}
## 9 Time Series Analysis 
library("ggplot2")
library("zoo")
library("forecast")

set.seed(12)
days <- seq(as.Date("2023-01-01"), by = "1 day", length.out = 365)
insulin_levels <- rnorm(365, mean = 95, sd = 10) # Hypothetical insulin hormone levels

# Preparing Time Series Object
insulin_time_series <- ts(insulin_levels, start = c(2023, 1), frequency = 365)
insulin_time_series

ggplot(data = data.frame(Day = days, Insulin = insulin_levels), aes(x = Day, y = Insulin)) +
  geom_line() +
  geom_hline(yintercept = mean(insulin_levels), color = "red", linetype = "solid") +
  scale_x_date(date_labels = "%m.%Y") +
  labs(title = "Insulin Hormone Levels Time Series",
       x = "Date",
       y = "Insulin Levels")

# Smoothed moving average for trend
insulin_trend <- rollmean(insulin_time_series, order = 7, k = 7) # 7-day moving average
plot(insulin_trend, type = "l", col = "red", lwd = 2, main = "Insulin Hormone Levels Trend")

# Seasonal subseries plot to visualise seasonal patterns
seasonplot(insulin_time_series, year.labels = FALSE, main = "Insulin Hormone Levels - Seasonal Subseries") # from libr forecast

# Autocorrelation function
acf(insulin_time_series, main = "Autocorrelation Function", ylim = c(-1, 1))

pacf(insulin_time_series, main = "Partial Autocorrelation Function")
```
Autocorrelation function is a tool that displays the correlation between consecutive observations in a time series data on a graph and is interpreted by examining how the correlation coefficient changes with the number of lags. An increase in the autocorrelation coefficient with the number of lags indicates positive serialisation in the data, while a decrease indicates negative serialisation.

Partial Autocorrelation Function is a tool that analyses the direct correlation between consecutive observations in a time series data one by one according to the number of lags. In the PACF graph, an increase in the autocorrelation coefficient with the number of lags indicates that there is "independent" positive serialisation in the data, while a decrease indicates "independent" negative serialisation. This shows how much the observations at that lag directly affect the next observation.

When the first graph is analysed, it is seen that there are noisy rises and falls from time to time. Especially around 08.2023 and 11.2023, two extreme decreases in daily insulin levels and an extreme increase around 11.2023 are observed.
It is observed that insulin levels remain at a certain average throughout the year.

In the Trend graph, which looks cleaner, the biggest rise is observed around 05.2023. The period when this person's insulin levels are most variable seems to be during April-June.

Looking at the seasonal subseries graph, it is seen that the changes observed in the first graph are also reflected in the seasonal graph.

The first large spike in the ACF graph indicates that the data may contain trends or cycles. In regions where lags increase, smaller signs of serialisation are observed.

In the PACF graph, negative and positive serialities are observed within the confidence intervals. The variability in the April-June period is represented by the decrease in negative autocorrelations in the PACF graph.

```{r}
## 10 Decision Tree for Predicting Cancer Diagnosis
library(rpart)
library(rpart.plot)

# Create an example dataset for cancer diagnosis (this example dataset is randomly generated)
set.seed(56)
data <- data.frame(
  Age = sample(30:70, 100, replace = TRUE),
  Gender = sample(c("Male", "Female"), 100, replace = TRUE), 
  Smoking = sample(c("Yes", "No"), 100, replace = TRUE), 
  Family_History = sample(c("Yes", "No"), 100, replace = TRUE), 
  Blood_Test = runif(100, 0, 1), # Hypothetical tumour marker values, between 0 and 1, higher value indicates higher risk
  Cancer = sample(c(0, 1), 100, replace = TRUE) # Cancer diagnosis, 0 or 1 
)

# Create a decision tree model
model <- rpart(Cancer ~ ., data = data, method = "class") # "Cancer" is the dependent variable, and "." represents all other variables as independent, method = "class" for classification problem

print(model)
rpart.plot(model)

# Try to predict whether the new patient has cancer using the created model
# Get characteristics of a new patient from user input
new_patient <- data.frame(
  Age = as.numeric(readline("Enter patient's age: ")),
  Gender = readline("Enter patient's gender (Male/Female): "),
  Smoking = readline("Does the patient smoke? (Yes/No): "),
  Family_History = readline("Is there a family history of cancer? (Yes/No): "),
  Blood_Test = as.numeric(readline("Enter patient's blood test result (0-1): "))
)

prediction <- predict(model, new_patient, type = "class")

if (prediction == 1) {
  print("The patient is likely to have cancer.")
} else {
  print("The patient is unlikely to have cancer.")
}

summary(model)
```
Firstly, the main threshold values into which the nodes of the decision tree model containing 100 nodes are divided are expressed.

22 Age, female gender, non-smoker, family history of cancer and 0.25 blood test result are predicted to be CANCEROUS.

"CP" (complexity parameter) is the parameter that controls the size of the tree. The smaller this parameter, the larger the tree. The separation of each node must result in a reduction greater than the value determined by the CP. A small CP can cause the tree to branch more, which can lead to overfitting of the model to the data and reduced generalisation performance.

Nsplit is the number of times the tree is split, relative error is the error rate of the tree on the original data set, xerror (cross-validated error) is the error rate estimated by the tree during the cross-validation process. Xerror allows us to evaluate the generalisation performance of the model. The decrease in the Xerror values of the model as the number of splits increases indicates that the model IMPROVES over time and the error rate DECREASES. Xstd represents the standard deviation of the cross validation error rate.

The variable importance section shows how significantly each variable affects the results. According to the results, the most important variables in predicting cancer diagnosis are age, blood test and family history. Gender and smoking are less important in the model.

Next, the characteristics of the root node are expressed. It shows the main divisions and points in this node and how much these divisions improve the prediction. The node has 100 observations and 50% accuracy (loss rate=0.5).

In the second node, the number of observations decreased to 21 and the accuracy of the predictions increased to 71%. In this node, the probability of a "0" result is 71.4% and the probability of a "1" result is 28.6%. The CP of the node also decreased. Surrogate splits are also located in this node to be used in case the main split is insufficient.

Node 27, at the very end of the output, contains 7 observations with a prediction of "1", all observations belonging to the value "1". It estimates the high probability of having cancer. Node 52 has 17 observations with an error rate of 29.41%, indicating that the probability of cancer is low ("0"). And node 53 contains 10 observations belonging to the "1" class with a loss rate of 0.3%. It indicates a high possibility of having cancer. 

The model has a low CP, xerror rates are slightly high but tending to decrease, and there are occasional imbalances between classes.
